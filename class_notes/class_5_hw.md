# åŸºç¡€ä½œä¸š

## é…ç½®lmdeployè¿è¡Œç¯å¢ƒ

> åˆ›å»ºç¯å¢ƒ

```sh
studio-conda -t lmdeploy -o pytorch-2.1.2
```

![create env](../attachment/InternLM2_homework5.assets/create_env.png)

```sh
conda activate lmdeploy

pip install lmdeploy[all]==0.3.0
```

![image-20240409103419359](../attachment/InternLM2_homework5.assets/install_lmdeploy.png)

## ä¸‹è½½internlm-chat-1.8bæ¨¡å‹

> InternStudioä½¿ç”¨è½¯è¿æ¥æ–¹å¼

```sh
ln -s /root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b /root/lmdeploy/models
```

![image-20240409104212472](../attachment/InternLM2_homework5.assets/ln_model.png)

> è‡ªå·±æœåŠ¡å™¨ä¸Šä¸‹è½½

```sh
apt install git-lfs
git lfs install  --system
git clone https://code.openxlab.org.cn/OpenLMLab/internlm2-chat-1.8b.git
```

![image-20240409105132175](../attachment/InternLM2_homework5.assets/down_model.png)

## ä½¿ç”¨Transformeråº“è¿è¡Œæ¨¡å‹

```python
import transformers
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import torch


print("torch version: ", torch.__version__)
print("transformers version: ", transformers.__version__)


model_dir = "./models/internlm2-chat-1_8b"
quantization = False

# tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=False, trust_remote_code=True)

# é‡åŒ–
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,                      # æ˜¯å¦åœ¨4ä½ç²¾åº¦ä¸‹åŠ è½½æ¨¡å‹ã€‚å¦‚æœè®¾ç½®ä¸ºTrueï¼Œåˆ™åœ¨4ä½ç²¾åº¦ä¸‹åŠ è½½æ¨¡å‹ã€‚
    load_in_8bit=False,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False,
    bnb_4bit_compute_dtype=torch.float16,   # 4ä½ç²¾åº¦è®¡ç®—çš„æ•°æ®ç±»å‹ã€‚è¿™é‡Œè®¾ç½®ä¸ºtorch.float16ï¼Œè¡¨ç¤ºä½¿ç”¨åŠç²¾åº¦æµ®ç‚¹æ•°ã€‚
    bnb_4bit_quant_type='nf4',              # 4ä½ç²¾åº¦é‡åŒ–çš„ç±»å‹ã€‚è¿™é‡Œè®¾ç½®ä¸º"nf4"ï¼Œè¡¨ç¤ºä½¿ç”¨nf4é‡åŒ–ç±»å‹ã€‚ nf4: 4bit-NormalFloat
    bnb_4bit_use_double_quant=True,         # æ˜¯å¦ä½¿ç”¨åŒç²¾åº¦é‡åŒ–ã€‚å¦‚æœè®¾ç½®ä¸ºTrueï¼Œåˆ™ä½¿ç”¨åŒç²¾åº¦é‡åŒ–ã€‚
)

# åˆ›å»ºæ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    model_dir,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    device_map='auto',
    low_cpu_mem_usage=True, # æ˜¯å¦ä½¿ç”¨ä½CPUå†…å­˜,ä½¿ç”¨ device_map å‚æ•°å¿…é¡»ä¸º True
    quantization_config=quantization_config if quantization else None,
)
model.eval()

# print(model.__class__.__name__) # InternLM2ForCausalLM

print(f"model.device: {model.device}, model.dtype: {model.dtype}")

system_prompt = """You are an AI assistant whose name is InternLM (ä¹¦ç”ŸÂ·æµ¦è¯­).
- InternLM (ä¹¦ç”ŸÂ·æµ¦è¯­) is a conversational language model that is developed by Shanghai AI Laboratory (ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤). It is designed to be helpful, honest, and harmless.
- InternLM (ä¹¦ç”ŸÂ·æµ¦è¯­) can understand and communicate fluently in the language chosen by the user such as English and ä¸­æ–‡.
"""
# system_prompt = "ä½ æ˜¯ä¸€ä¸ªå†œä¸šä¸“å®¶ï¼Œè¯·å‡†ç¡®å›ç­”å†œä¸šç›¸å…³çš„é—®é¢˜"
print("system_prompt: ", system_prompt)


history = []
while True:
    query = input("è¯·è¾“å…¥æç¤º: ")
    query = query.replace(' ', '')
    if query == None or len(query) < 1:
        continue
    if query.lower() == "exit":
        break

    print("å›ç­”: ", end="")
    # https://huggingface.co/internlm/internlm2-chat-1_8b/blob/main/modeling_internlm2.py#L1185
    # stream_chat è¿”å›çš„å¥å­é•¿åº¦æ˜¯é€æ¸è¾¹é•¿çš„,lengthçš„ä½œç”¨æ˜¯è®°å½•ä¹‹å‰çš„è¾“å‡ºé•¿åº¦,ç”¨æ¥æˆªæ–­ä¹‹å‰çš„è¾“å‡º
    length = 0
    for response, history in model.stream_chat(
            tokenizer = tokenizer,
            query = query,
            history = history,
            max_new_tokens = 1024,
            do_sample = True,
            temperature = 0.8,
            top_p = 0.8,
            meta_instruction = system_prompt,
        ):
        if response is not None:
            print(response[length:], flush=True, end="")
            length = len(response)
    print("\n")
```

> è¿è¡Œå‘½ä»¤è®°å½•

```sh
(lm) root@intern-studio-030876:~/lmdeploy# python internlm2_chat_1_8b_load_stream_chat.py 
torch version:  2.1.2
transformers version:  4.37.2
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:01<00:00, 30.93s/it]
model.device: cuda:0, model.dtype: torch.bfloat16
system_prompt:  You are an AI assistant whose name is InternLM (ä¹¦ç”ŸÂ·æµ¦è¯­).
- InternLM (ä¹¦ç”ŸÂ·æµ¦è¯­) is a conversational language model that is developed by Shanghai AI Laboratory (ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤). It is designed to be helpful, honest, and harmless.
- InternLM (ä¹¦ç”ŸÂ·æµ¦è¯­) can understand and communicate fluently in the language chosen by the user such as English and ä¸­æ–‡.

è¯·è¾“å…¥æç¤º: è¯·ç»™æˆ‘è®²ä¸€ä¸ªå…³äºçŒ«å’Œè€é¼ çš„å°æ•…äº‹
å›ç­”: å¥½çš„ï¼Œä»¥ä¸‹æ˜¯å…³äºçŒ«å’Œè€é¼ çš„å°æ•…äº‹ï¼š

ä»å‰ï¼Œæœ‰ä¸€åªåå«æ±¤å§†çš„çŒ«ï¼Œä»–éå¸¸å–œæ¬¢å·è€é¼ ã€‚æœ‰ä¸€å¤©ï¼Œæ±¤å§†æŠ“åˆ°äº†ä¸€åªè€é¼ ï¼Œä»–éå¸¸å…´å¥‹ï¼Œå†³å®šæŠŠè¿™ä¸ªè€é¼ ä½œä¸ºä»–çš„æˆ˜åˆ©å“ã€‚ä»–æŠŠå®ƒå¸¦å›å®¶ï¼Œå‡†å¤‡å¥½å¥½åœ°äº«å—ä¸€ä¸‹è¿™ä¸ªç¾å‘³çš„æ™šé¤ã€‚

ä½†æ˜¯ï¼Œå½“ä»–æ‰“å¼€ç¬¼å­çš„æ—¶å€™ï¼Œå´å‘ç°è¿™åªè€é¼ ä¸è§äº†ï¼ä»–å¼€å§‹å››å¤„å¯»æ‰¾ï¼Œä½†æ˜¯æ‰¾ä¸åˆ°è€é¼ çš„è¸ªè¿¹ã€‚ä»–å¼€å§‹æ„Ÿåˆ°å¾ˆæ²®ä¸§ï¼Œè§‰å¾—è‡ªå·±å¯èƒ½åšé”™äº†ä»€ä¹ˆã€‚

å°±åœ¨è¿™æ—¶ï¼Œæ±¤å§†çœ‹åˆ°äº†ä»–çš„å¥½å‹â€”â€”ä¸€åªå«åšæ°å…‹çš„çŒ«ã€‚æ°å…‹å‘Šè¯‰æ±¤å§†ï¼Œä»–çŸ¥é“è€é¼ åœ¨å“ªé‡Œï¼Œä½†æ˜¯æ±¤å§†å¿…é¡»ç­”åº”ä»–ä¸€ä»¶äº‹æƒ…ã€‚å¦‚æœæ±¤å§†èƒ½å¤Ÿå¸®åŠ©æ°å…‹æ‰ä½è€é¼ ï¼Œä»–å¯ä»¥æˆä¸ºæ°å…‹çš„æœ‹å‹ã€‚

æ±¤å§†æƒ³äº†æƒ³ï¼Œè§‰å¾—è¿™æ˜¯ä¸€ä¸ªæœºä¼šã€‚ä»–ç­”åº”æ°å…‹ï¼Œå¦‚æœä»–èƒ½æ‰ä½è€é¼ ï¼Œä»–å°±ä¼šæˆä¸ºä»–çš„æœ‹å‹ã€‚æ°å…‹åŒæ„äº†ï¼Œä»–ä»¬å¼€å§‹åˆä½œï¼Œä¸€èµ·å¯»æ‰¾è€é¼ ã€‚

ç»è¿‡å‡ å¤©çš„åŠªåŠ›ï¼Œä»–ä»¬ç»ˆäºæ‰¾åˆ°äº†è€é¼ ã€‚æ±¤å§†éå¸¸é«˜å…´ï¼Œä»–å†³å®šè®©æ°å…‹æˆä¸ºä»–çš„æœ‹å‹ã€‚æ°å…‹éå¸¸é«˜å…´ï¼Œä»–ä»¬ä¸€èµ·åº¦è¿‡äº†æ„‰å¿«çš„æ—¶å…‰ã€‚

ä»é‚£å¤©èµ·ï¼Œæ±¤å§†å’Œæ°å…‹æˆä¸ºäº†æœ€å¥½çš„æœ‹å‹ï¼Œä»–ä»¬ä¸€èµ·ç©è€ï¼Œåˆ†äº«å¿«ä¹å’Œæ‚²ä¼¤ã€‚è¿™ä¸ªæ•…äº‹å‘Šè¯‰æˆ‘ä»¬ï¼Œå‹è°Šæ˜¯ä¸€ç§å®è´µçš„è´¢å¯Œï¼Œåªæœ‰çœŸå¿ƒå¯¹å¾…æœ‹å‹ï¼Œæ‰èƒ½è·å¾—çœŸæ­£çš„å‹è°Šã€‚

è¯·è¾“å…¥æç¤º: exit
(lm) root@intern-studio-030876:~/lmdeploy#
```

![](../attachment/InternLM2_homework5.assets/transformers_run.png)

## ä½¿ç”¨å‘½ä»¤è¡Œæ–¹å¼ä¸æ¨¡å‹å¯¹è¯

```sh
# ä½¿ç”¨pytorchåç«¯
lmdeploy chat \
    models/internlm2-chat-1_8b \
    --backend pytorch

# ä½¿ç”¨turbomindåç«¯
lmdeploy chat \
    models/internlm2-chat-1_8b \
    --backend turbomind
```

> å‘½ä»¤è¿è¡Œè®°å½•

```sh
(lm) root@intern-studio-030876:~/lmdeploy# lmdeploy chat \
>     models/internlm2-chat-1_8b \
>     --backend turbomind
2024-04-13 18:16:26,536 - lmdeploy - WARNING - model_source: hf_model
2024-04-13 18:16:26,538 - lmdeploy - WARNING - kwargs max_batch_size is deprecated to initialize model, use TurbomindEngineConfig instead.
2024-04-13 18:16:26,538 - lmdeploy - WARNING - kwargs cache_max_entry_count is deprecated to initialize model, use TurbomindEngineConfig instead.
2024-04-13 18:16:29,924 - lmdeploy - WARNING - model_config:

[llama]
model_name = internlm2
tensor_para_size = 1
head_num = 16
kv_head_num = 8
vocab_size = 92544
num_layer = 24
inter_size = 8192
norm_eps = 1e-05
attn_bias = 0
start_id = 1
end_id = 2
session_len = 32776
weight_type = bf16
rotary_embedding = 128
rope_theta = 1000000.0
size_per_head = 128
group_size = 0
max_batch_size = 128
max_context_token_num = 1
step_length = 1
cache_max_entry_count = 0.8
cache_block_seq_len = 64
cache_chunk_size = -1
num_tokens_per_iter = 0
max_prefill_iters = 1
extra_tokens_per_iter = 0
use_context_fmha = 1
quant_policy = 0
max_position_embeddings = 32768
rope_scaling_factor = 0.0
use_dynamic_ntk = 0
use_logn_attn = 0


2024-04-13 18:16:30,965 - lmdeploy - WARNING - get 195 model params
2024-04-13 18:16:54,643 - lmdeploy - WARNING - Input chat template with model_name is None. Forcing to use internlm2                                                      
[WARNING] gemm_config.in is not found; using default GEMM algo
session 1

double enter to end input >>> è¯·ç»™æˆ‘è®²ä¸€ä¸ªå…³äºçŒ«å’Œè€é¼ çš„å°æ•…äº‹

<|im_start|>system
You are an AI assistant whose name is InternLM (ä¹¦ç”ŸÂ·æµ¦è¯­).
- InternLM (ä¹¦ç”ŸÂ·æµ¦è¯­) is a conversational language model that is developed by Shanghai AI Laboratory (ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤). It is designed to be helpful, honest, and harmless.
- InternLM (ä¹¦ç”ŸÂ·æµ¦è¯­) can understand and communicate fluently in the language chosen by the user such as English and ä¸­æ–‡.
<|im_end|>
<|im_start|>user
è¯·ç»™æˆ‘è®²ä¸€ä¸ªå…³äºçŒ«å’Œè€é¼ çš„å°æ•…äº‹<|im_end|>
<|im_start|>assistant
 2024-04-13 18:17:50,248 - lmdeploy - WARNING - kwargs ignore_eos is deprecated for inference, use GenerationConfig instead.
2024-04-13 18:17:50,248 - lmdeploy - WARNING - kwargs random_seed is deprecated for inference, use GenerationConfig instead.
å½“ç„¶ï¼Œæˆ‘å¾ˆä¹æ„ç»™ä½ è®²ä¸€ä¸ªå…³äºçŒ«å’Œè€é¼ çš„å°æ•…äº‹ã€‚

ä»å‰ï¼Œæœ‰ä¸€åªéå¸¸èªæ˜çš„è€é¼ å’Œä¸€åªéå¸¸å–„äºæ‰è€é¼ çš„çŒ«ã€‚è€é¼ å’ŒçŒ«çš„ç”Ÿæ´»æ€»æ˜¯å……æ»¡äº†ä¹è¶£å’ŒæŒ‘æˆ˜ã€‚

æœ‰ä¸€å¤©ï¼Œå½“è€é¼ å‘ç°çŒ«çš„æ•çŒæŠ€å·§æ—¶ï¼Œå®ƒå†³å®šæƒ³å‡ºä¸€ç§æ™ºæ…§çš„æˆ˜ç•¥æ¥å¯¹æŠ—çŒ«ã€‚è€é¼ å†³å®šå°†è‡ªå·±è—åœ¨ä¸€ä¸ªéå¸¸å®‰å…¨çš„åœ°æ–¹ï¼Œç­‰å¾…çŒ«çš„åˆ°æ¥ã€‚å½“çŒ«å‡†å¤‡è¿›å…¥è€é¼ çš„è—èº«å¤„æ—¶ï¼Œè€é¼ çªç„¶è·³å‡ºæ¥ï¼Œå°†çŒ«çš„çˆªå­å¼„å¾—â€œå˜å±å˜å±â€å“ã€‚

çŒ«çš„è‚šå­ç–¼å¾—ä¸å¾—äº†ï¼Œå®ƒæ— æ³•ç»§ç»­è¿½æ•è€é¼ ã€‚è€é¼ è¶è¿™ä¸ªæœºä¼šï¼Œè¿…é€Ÿæºœèµ°ï¼Œèº²åˆ°äº†ä¸€ä¸ªå®‰å…¨çš„åœ°æ–¹ã€‚çŒ«æ„Ÿåˆ°å›°æƒ‘å’Œæ²®ä¸§ï¼Œå®ƒä¸çŸ¥é“å‘ç”Ÿäº†ä»€ä¹ˆäº‹ã€‚

å‡ å¤©åï¼ŒçŒ«å¶ç„¶å‘ç°äº†ä¸€ä¸ªè€é¼ æ´ï¼Œå‘ç°è€é¼ æ—©å°±ç¦»å¼€äº†ã€‚çŒ«æ„Ÿåˆ°éå¸¸å¤±æœ›ï¼Œå¿ƒæƒ³å®ƒä¸€å®šæ˜¯å·äº†åˆ«çš„è€é¼ çš„é£Ÿç‰©ï¼Œç„¶åæ‰ç¦»å¼€çš„ã€‚

çŒ«å¼€å§‹æ„Ÿåˆ°æ„¤æ€’å’Œæ²®ä¸§ã€‚å®ƒå¼€å§‹ç”¨å®ƒçš„æ•çŒæŠ€å·§æ¥è¿½æ•è€é¼ ï¼Œä½†ä¸è®ºå®ƒæ€ä¹ˆåŠªåŠ›ï¼Œè€é¼ éƒ½æ€»æ˜¯èƒ½å¤Ÿé€ƒè„±çŒ«çš„è¿½å‡»ã€‚

æ¸æ¸åœ°ï¼ŒçŒ«æ¸æ¸å¤±å»äº†è€å¿ƒã€‚åœ¨è€é¼ çš„æ´é‡Œï¼Œå®ƒæ„Ÿåˆ°éå¸¸å­¤ç‹¬å’Œæ— åŠ©ã€‚

ä¸€å¤©ï¼Œè€é¼ çœ‹åˆ°äº†çŒ«çš„å›°å¢ƒï¼Œå®ƒå†³å®šå¸®åŠ©çŒ«ã€‚è€é¼ å‘Šè¯‰çŒ«ï¼Œå®ƒçŸ¥é“çŒ«æœ€å–œæ¬¢çš„é£Ÿç‰©æ˜¯é±¼ï¼Œæ‰€ä»¥å®ƒæƒ³å‡ºäº†ä¸€ä¸ªå·§å¦™çš„æ–¹æ³•ï¼Œè®©çŒ«å»æŠ“é±¼ï¼Œç„¶åè‡ªå·±å°±å¯ä»¥å®‰å…¨åœ°åƒé¥­äº†ã€‚

çŒ«å¬ä»äº†è€é¼ çš„å»ºè®®ï¼Œå»æŠ“é±¼ã€‚ä½†è€é¼ å¹¶æ²¡æœ‰è®©çŒ«æ‰åˆ°é±¼ï¼Œå®ƒåˆ©ç”¨è‡ªå·±çš„çµæ´»æŠ€å·§ï¼ŒæŠŠçŒ«æ‹‰å›æ´é‡Œï¼Œè‡ªå·±æŠ“åˆ°äº†é±¼ã€‚

è€é¼ å’ŒçŒ«ä»æ­¤ä¸€èµ·äº«å—ç¾é£Ÿå’Œå†’é™©ï¼Œå®ƒä»¬æˆä¸ºäº†å¥½æœ‹å‹ï¼Œå…±åŒåº¦è¿‡äº†è®¸å¤šç¾å¥½çš„æ—¶å…‰ã€‚

è¿™ä¸ªæ•…äº‹å‘Šè¯‰æˆ‘ä»¬ï¼Œæ™ºæ…§å’Œè€å¿ƒæ˜¯æˆ˜èƒœä»»ä½•å›°éš¾çš„å…³é”®ã€‚æœ‰æ—¶å€™ï¼Œæˆ‘ä»¬éœ€è¦è·³å‡ºè‡ªå·±çš„èˆ’é€‚åŒºï¼Œå»å°è¯•æ–°çš„äº‹ç‰©ã€‚é€šè¿‡æ™ºæ…§å’Œåˆä½œï¼Œæˆ‘ä»¬å¯ä»¥å®ç°æ›´å¤§çš„æˆåŠŸã€‚

double enter to end input >>> EXIT


<|im_start|>user
EXIT<|im_end|>
<|im_start|>assistant
 å¯¹ä¸èµ·ï¼Œæˆ‘æ— æ³•ç†è§£æ‚¨çš„é—®é¢˜ã€‚å¦‚æœ‰å…¶ä»–é—®é¢˜ï¼Œæ¬¢è¿éšæ—¶å‘æˆ‘æé—®ï¼Œæˆ‘ä¼šåœ¨æˆ‘èƒ½åŠ›èŒƒå›´å†…å°½åŠ›ä¸ºæ‚¨è§£ç­”ã€‚

double enter to end input >>> exit

(lm) root@intern-studio-030876:~/lmdeploy#
```

![](../attachment/InternLM2_homework5.assets/chat1.png)

# è¿›é˜¶ä½œä¸š

## W4A16é‡åŒ–

```sh
lmdeploy lite auto_awq \
  models/internlm2-chat-1_8b \
  --calib-dataset 'ptb' \
  --calib-samples 128 \
  --calib-seqlen 1024 \
  --w-bits 4 \
  --w-group-size 128 \
  --work-dir models/internlm2-chat-1_8b-4bit
```

> å‘½ä»¤è¿è¡Œè®°å½•

![](../attachment/InternLM2_homework5.assets/w4a16.png)

## KV Cache=0.4 W4A16 å‘½ä»¤è¡Œ

è®¾ç½®KV Cacheæœ€å¤§å ç”¨æ¯”ä¾‹ä¸º0.4ï¼Œå¼€å¯W4A16é‡åŒ–ï¼Œä»¥å‘½ä»¤è¡Œæ–¹å¼ä¸æ¨¡å‹å¯¹è¯ã€‚

```sh
lmdeploy chat \
    models/internlm2-chat-1_8b-4bit \
    --backend turbomind \
    --model-format awq \
    --cache-max-entry-count 0.4
```

> å‘½ä»¤è¿è¡Œè®°å½•

```sh
(lm) root@intern-studio-030876:~/lmdeploy# lmdeploy chat \
>     models/internlm2-chat-1_8b-4bit \
>     --backend turbomind \
>     --model-format awq \
>     --cache-max-entry-count 0.4
2024-04-13 18:21:27,228 - lmdeploy - WARNING - model_source: hf_model
2024-04-13 18:21:27,228 - lmdeploy - WARNING - kwargs model_format is deprecated to initialize model, use TurbomindEngineConfig instead.
2024-04-13 18:21:27,228 - lmdeploy - WARNING - kwargs max_batch_size is deprecated to initialize model, use TurbomindEngineConfig instead.
2024-04-13 18:21:27,228 - lmdeploy - WARNING - kwargs cache_max_entry_count is deprecated to initialize model, use TurbomindEngineConfig instead.
2024-04-13 18:21:33,984 - lmdeploy - WARNING - model_config:

[llama]
model_name = internlm2
tensor_para_size = 1
head_num = 16
kv_head_num = 8
vocab_size = 92544
num_layer = 24
inter_size = 8192
norm_eps = 1e-05
attn_bias = 0
start_id = 1
end_id = 2
session_len = 32776
weight_type = int4
rotary_embedding = 128
rope_theta = 1000000.0
size_per_head = 128
group_size = 128
max_batch_size = 128
max_context_token_num = 1
step_length = 1
cache_max_entry_count = 0.4
cache_block_seq_len = 64
cache_chunk_size = -1
num_tokens_per_iter = 0
max_prefill_iters = 1
extra_tokens_per_iter = 0
use_context_fmha = 1
quant_policy = 0
max_position_embeddings = 32768
rope_scaling_factor = 0.0
use_dynamic_ntk = 0
use_logn_attn = 0


2024-04-13 18:21:35,171 - lmdeploy - WARNING - get 267 model params
2024-04-13 18:22:14,979 - lmdeploy - WARNING - Input chat template with model_name is None. Forcing to use internlm2
[WARNING] gemm_config.in is not found; using default GEMM algo
session 1

double enter to end input >>> è¯·ç»™æˆ‘è®²ä¸€ä¸ªå…³äºçŒ«å’Œè€é¼ çš„å°æ•…äº‹

<|im_start|>system
You are an AI assistant whose name is InternLM (ä¹¦ç”ŸÂ·æµ¦è¯­).
- InternLM (ä¹¦ç”ŸÂ·æµ¦è¯­) is a conversational language model that is developed by Shanghai AI Laboratory (ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤). It is designed to be helpful, honest, and harmless.
- InternLM (ä¹¦ç”ŸÂ·æµ¦è¯­) can understand and communicate fluently in the language chosen by the user such as English and ä¸­æ–‡.
<|im_end|>
<|im_start|>user
è¯·ç»™æˆ‘è®²ä¸€ä¸ªå…³äºçŒ«å’Œè€é¼ çš„å°æ•…äº‹<|im_end|>
<|im_start|>assistant
 2024-04-13 18:24:11,517 - lmdeploy - WARNING - kwargs ignore_eos is deprecated for inference, use GenerationConfig instead.
2024-04-13 18:24:11,517 - lmdeploy - WARNING - kwargs random_seed is deprecated for inference, use GenerationConfig instead.
å½“ç„¶ï¼Œå¯ä»¥ä¸ºæ‚¨åˆ†äº«ä¸€ä¸ªç»å…¸çš„è€é¼ ä¸çŒ«çš„æ•…äº‹æƒ…èŠ‚ã€‚

ä»å‰ï¼Œæœ‰ä¸€ä¸ªå°è€é¼ åå«æ±¤å§†ï¼ˆTomï¼‰ï¼Œä»–ä½åœ¨ä¸€å®¶å°é¤é¦†é‡Œã€‚æ±¤å§†ä¸å¨å¸ˆç»“ä¸‹äº†ä¸€ä¸ªä¸å¹³å‡¡çš„å‹è°Šï¼Œä»–ç»å¸¸å‘ä»–å±•ç¤ºè‡ªå·±ç²¾æ¹›çš„åˆ‡èœæŠ€æœ¯ã€‚ä»–çš„æŠ€æœ¯ç”šè‡³å¾æœäº†é¤é¦†è€æ¿ç±³å‹’ï¼ˆMr. Millsï¼‰çš„ç‹¬è£ã€‚

ä½†æœ‰ä¸€å¤©ï¼Œç±³å‹’çš„å¤ªå¤ªè‰è‰ï¼ˆLilyï¼‰å†³å®šåœ¨é¤å…ä¸¾åŠä¸€åœºç››å¤§çš„åº†ç”Ÿä¼šï¼Œé‚€è¯·æ±¤å§†åˆ†äº«å¥¹çš„åº†ç¥å®´å¸­ã€‚æ±¤å§†å¾ˆé«˜å…´èƒ½å¤Ÿå‘ä»–çš„æœ‹å‹å±•ç¤ºä»–çš„åˆ€å·¥ï¼Œä½†ä»–çš„æŠ€æœ¯å´å› è‰è‰çš„åˆ°æ¥è€Œå˜å¾—ç´§å¼ èµ·æ¥ã€‚

è‰è‰æ˜¯ä¸€ä¸ªèªæ˜ç‹¡é» çš„å¥³äººï¼Œå¥¹çŸ¥é“æ±¤å§†å¯¹å¨å¸ˆæœ‰åè¢’ï¼Œå› æ­¤å¥¹æå‡ºä¸¾åŠä¸€åœºçŒ«æ‰è€é¼ çš„é—¹å‰§æ¥åˆ†æ•£æ±¤å§†çš„æ³¨æ„åŠ›ã€‚è‰è‰æå‡ºï¼Œå¦‚æœä»–ä»¬èƒ½å¤ŸæˆåŠŸï¼Œå¥¹å°†å¥–åŠ±æ±¤å§†ä¸€ä¸ªç‰¹åˆ«çš„ç¤¼ç‰©ã€‚

æ±¤å§†æ¥å—äº†è‰è‰çš„æè®®ï¼Œä»–å¾ˆé«˜å…´èƒ½å‘ä»–çš„æœ‹å‹å±•ç¤ºä»–çš„å¨è‰ºã€‚ä»–ç²¾å¿ƒè®¾è®¡äº†ä¸€ä¸ªçŒ«æ‰è€é¼ çš„è®¡åˆ’ï¼Œå¹¶ç¡®ä¿ä»–å’Œè‰è‰éƒ½æœ‰æœºä¼šå‚ä¸å…¶ä¸­ã€‚

æ±¤å§†å’Œè‰è‰ä¸€èµ·å‡†å¤‡äº†é£Ÿç‰©ï¼Œä»–ä»¬è®¾ä¸‹é™·é˜±ï¼Œç­‰å¾…è€é¼ çš„åˆ°æ¥ã€‚è€Œæ±¤å§†åˆ™æ‘†å¥½äº†ä»–çš„åˆ‡èœç¢—ï¼Œå‡†å¤‡äº«ç”¨è¿™ä¸ªä»–ç²¾å¿ƒå‡†å¤‡çš„åº†ç”Ÿå®´å¸­ã€‚

å½“è‰è‰å’Œæ±¤å§†è¿›å…¥é¤é¦†æ—¶ï¼Œä»–ä»¬ä¹‹é—´å‘ç”Ÿäº†æ¿€çƒˆçš„çŒ«æ‰è€é¼ çš„æ··ä¹±ã€‚æ±¤å§†çš„åˆ€å·¥æŠ€å·§ä»¤çŒ«é™·å…¥ä»–çš„é™·é˜±ä¹‹ä¸­ï¼Œè€Œè‰è‰åˆ™è®¾è®¡äº†ä¸€ä¸ªå·§å¦™çš„å°çŒ«é™·é˜±ï¼Œå°†å¥¹çš„å°è€é¼ å¸å¼•åˆ°æ±¤å§†çš„åˆ‡èœç¢—é‡Œã€‚

æ±¤å§†å’Œè‰è‰çš„è®¡åˆ’æˆåŠŸäº†ï¼Œä»–ä»¬æˆåŠŸåœ°æ‰åˆ°äº†ä¸€åªæ¸©é¡ºçš„è€é¼ ï¼Œä½†ä»–ä»¬ä¹Ÿæˆä¸ºäº†æœ‹å‹ã€‚æ±¤å§†æ„è¯†åˆ°è‰è‰çš„èªæ˜æ‰æ™ºå’Œè‰è‰å¯¹ä»–çš„æ¬£èµè®©ä»–æ„Ÿåˆ°æ— æ¯”æ»¡è¶³å’Œå¿«ä¹ã€‚ä»–ä»¬å¼€å§‹ä¸€èµ·çƒ¹é¥ªç¾é£Ÿï¼Œä»–ä»¬çš„å‹è°Šä¹Ÿå˜å¾—æ›´åŠ åšå›ºã€‚

ä»é‚£ä¸€åˆ»èµ·ï¼Œæ±¤å§†å’Œè‰è‰æˆä¸ºäº†å¥½æœ‹å‹ï¼Œæ±¤å§†ç”¨ä»–çš„å¨è‰ºä¸ºè‰è‰çš„åº†ç”Ÿå®´æä¾›äº†æ— æ•°çš„æƒŠå–œå’Œæ¸©æš–ã€‚ä»–ä»¬æˆä¸ºäº†ä¸€ä¸ªèªæ˜çš„çŒ«å’Œä¸€ä¸ªèªæ˜çš„çŒ«çš„æ•…äº‹ã€‚

double enter to end input >>> exit

(lm) root@intern-studio-030876:~/lmdeploy#
```

![](../attachment/InternLM2_homework5.assets/chat2.png)

## API Server W4A16é‡åŒ– KV Cache=0.4

ä»¥API Serveræ–¹å¼å¯åŠ¨ lmdeployï¼Œå¼€å¯ W4A16é‡åŒ–ï¼Œè°ƒæ•´KV Cacheçš„å ç”¨æ¯”ä¾‹ä¸º0.4ï¼Œåˆ†åˆ«ä½¿ç”¨å‘½ä»¤è¡Œå®¢æˆ·ç«¯ä¸Gradioç½‘é¡µå®¢æˆ·ç«¯ä¸æ¨¡å‹å¯¹è¯ã€‚

### server

> å¯åŠ¨æœåŠ¡

```sh
lmdeploy serve api_server \
    models/internlm2-chat-1_8b-4bit \
    --backend turbomind \
    --model-format awq \
    --tp 1 \
    --cache-max-entry-count 0.4 \
    --quant-policy 0 \
    --model-name internlm2_1_8b_chat \
    --server-name 0.0.0.0 \
    --server-port 23333
```

![image-20240409141039295](../attachment/InternLM2_homework5.assets/server1.png)

> ç«¯å£è®¿é—®

```sh
lmdeploy serve api_client http://localhost:23333
```

![image-20240409141247373](../attachment/InternLM2_homework5.assets/server2.png)

> è¿œç¨‹è¿æ¥

```SH
ssh -CNg -L 23333:127.0.0.1:23333 root@ssh.intern-ai.org.cn -p 40165
```

> è®¿é—® `127.0.0.1:23333`

![server3](../attachment/InternLM2_homework5.assets/server3.jpeg)

> è®¿é—® `/v1/chat/completions`

```json
{
  "model": "internlm2_1_8b_chat",
  "messages": [
    {
      "content": "ç»™æˆ‘è®²ä¸€ä¸ªçŒ«å’Œè€é¼ çš„æ•…äº‹",
      "role": "user"
    }
  ],
  "temperature": 0.8,
  "top_p": 0.8,
  "n": 1,
  "max_tokens": null,
  "stop": null,
  "stream": false,
  "presence_penalty": 0,
  "frequency_penalty": 0,
  "user": "string",
  "repetition_penalty": 2,
  "session_id": -1,
  "ignore_eos": false,
  "skip_special_tokens": true,
  "top_k": 40
}
```

> æ•ˆæœå¹¶ä¸ç†æƒ³ï¼Œè°ƒæ•´ `temperature`, `top_p`, `presence_penalty`, `frequency_penalty`, `repetition_penalty`, `top_k` ä¹Ÿæ²¡æœ‰æ˜æ˜¾å˜åŒ–

```json
{
  "id": "10",
  "object": "chat.completion",
  "created": 1712645394,
  "model": "internlm2_1_8b_chat",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "å°é¼ å®¶è¦ä¸¾åŠä¸€åœºç››å¤§çš„èšä¼šï¼Œå¯æ˜¯ä¸»äººå¤ªå¿™äº†ã€‚ä»–åªå¥½è¯·æ¥å¥½æœ‹å‹å¤§ä¸ªå­ã€äºŒå£®å’Œå°èƒ–å„¿ä¸€èµ·å‚åŠ è¿™æ¬¡ç››å®´çš„å®¢äººåå•é‡Œæœ‰ä¸€åªéå¸¸è°ƒçš®çš„å°åŠ¨ç‰©â€”â€”ä¸€åªåå«â€œè€ç‹¼â€çš„è€è™!è€è™æ˜¯è¿™é‡Œçš„ä¸»äººä¹‹ä¸€,å…¶å®ƒæˆå‘˜è¿˜æœ‰ä¸€åªæœ‰ç€ä¸€é¢—ç«çº¢å¿ƒçš„å¤§å¤´è›‡;å¦å¤–å‡ åéƒ½æ˜¯ä¸€äº›é¡½çš®çš„å­©å­ä»¬â€¦â€¦\næ—¶é—´è¿‡å¾—çœŸå¿«å•Š!\nè½¬çœ¼é—´åˆ°äº†æ­£é¤çš„æ—¶é—´å•¦!!\nä¸»äººå®¶å·²ç»æ‘†å¥½äº†ä¸€æ¡Œä¸°ç¾å¯å£çš„é¥­èœä¾›å®¢äººä»¬äº«ç”¨äº†èµ·æ¥.\nå¯è¿™äº›å¥½åƒçš„ä¸œè¥¿å´è®©é‚£äº›å­©å­ä»¬éƒ½åƒå¾—æ´¥ä¸ç›–è¿‡å‘³å‘¢!!çœ‹æ¥ä»–ä»¬å¾—æ‰¾äº›åˆ«çš„åŠæ³•æ¶ˆç£¨ä¸€ä¸‹è‡ªå·±çš„è‚šå­äº†å§?\nå°±åœ¨è¿™æ—¶.å¤§å®¶æ­£åœ¨å¼€å¿ƒåœ°åƒç€ç¾é£Ÿçš„æ—¶å€™ å¿½ç„¶ä»é™¢å­é‡Œä¼ æ¥ä¸€é˜µé˜µå–µå‘œå£°,åªè§é‚£åªæœ€è°ƒçš®çš„çŒ´å­ä¹Ÿæ¥åˆ°äº†è¿™å„¿çš„é—¨å£æŠŠé—¨å¼€äº†å¼€æ¥!å“ˆå“ˆâ€¦åŸæ¥ä»–æ˜¯æ¥æ‰¾æˆ‘ä»¬çš„å‘€!!!!å¤©å“ªï¼éš¾é“è¿™å°±æ˜¯ä¼ è¯´ä¸­çš„é‚€è¯·å‡½å—ï¼Ÿæˆ‘ä»¬æ˜¯ä¸æ˜¯è¯¥èµ¶ç´§èº²åˆ°æ¡Œå­åº•ä¸‹å»?è¿˜æ˜¯åˆ«ç®¡é—²äº‹äº†??è¿™æ ·å§ï¼šå’±ä»¬å°±å…ˆæŠŠä»Šå¤©æ™šä¸Šçš„æ¸¸æˆè§„åˆ™è¯´ä¸€è¯´å†è¡Œç©è€å‘—!!!å”‰å“Ÿ~å¯¹ä¸ä½å„ä½æœ‹å‹æ‰“æ‰°æ‚¨å®è´µçš„å·¥ä½œæ—¶é—´æ¥å¼€è¿™ç§ç©ç¬‘ä¼šä¸æ˜¯æˆ‘çš„æœ¬æ„å“¦,\nä½†æ˜¯æ²¡åŠæ³•è°å«ä½ ä»¬è¿™ä¹ˆè´ªç©å˜›???è€Œä¸”æˆ‘è¿˜è¦å‘é¢†å¯¼æ±‡æŠ¥å·¥ä½œå“©..å“å‘¦å¦ˆè€¶...æ€ä¹ˆåˆæ˜¯ä¸€é¡¿æ—©é¤æ¥äº†.....ä¸è¿‡è¯åˆè¯´å›æ¥..\nå—¯å“¼--æˆ‘æ˜¯ä¸ä¼šåƒè¿™ä¸ªèœçš„å“¦---è™½ç„¶å‘³é“å¾ˆé¦™~~ä½†æ˜¯æˆ‘çœŸçš„åƒä¸ä¸‹äº†...\nå’¦â€”æ˜¯è°åœ¨å«æˆ‘å§å§ä½ å‘Šè¯‰æˆ‘ä¸€å£°å¹²ä»€ä¹ˆå»äº†?!ç­‰ç­‰æ…¢ç‚¹æ…¢æ…¢çš„è¯´å‡ºæ¥å¥½ä¸å¥½ä¹ˆğŸ˜ƒâœ¿(Â°âˆ€Ëã‰™)â˜†_â‹¯_\nå¯¹äº†å®ƒåˆ°åº•é•¿ä»€ä¹ˆæ ·å­å‘????å¥½åƒæ²¡æœ‰å¬è¯´å®ƒçš„åå­—ä¸€æ ·è¯¶,,é‚£ä¸€å®šæ˜¯é•¿å¾—æ€ªæ€ªçš„æ‰å¯¹å§....ç°åœ¨å…ˆè®©æˆ‘ä»¬å¥½å¥½æ¬£èµä¸€ä¸‹å§:  ç§è§æ²¡è¿™åªå¯çˆ±çš„å°å®¶ä¼™å°±æ˜¯å¤§åé¼åçš„\"é¦‹å˜´ç‹\"\"é€—é€¼å¤§ç‹\",\"æƒ¹ç¥¸é¬¼\".è¢«å…¶ä»–å°æœ‹å‹äº²åˆ‡ç§°ä¸º''ä¸‰èŠ±'â€™â€˜å››æœµè‰',å®ƒæ˜¯é‚£ä¹ˆå¯çˆ±è¿·äººä½†åŒæ—¶ä¹Ÿå¾ˆæ·˜æ°”èªæ˜èƒ½å¹²ç‰¹åˆ«æœºçµè€Œå…·æœ‰é«˜è¶…è¡¨æ¼”å¤©èµ‹ä¸æƒŠäººçš„èˆè¹ˆæŠ€å·§åŠç²¾æ¹›çš„è‰ºæœ¯é€ è¯£ä»¥åŠå“è¶Šçš„å¤–å½¢è®¾è®¡åˆ›é€ åŠ›æ›´å…¼æœ‰ç‹¬ç‰¹ä¸”éå‡¡çš„äººæ ¼é­…åŠ›ä½¿å®ƒåœ¨ä¼—å¤šæ˜æ˜Ÿä¸­ç‹¬æ ‘ä¸ºå°Šå¹¶æˆä¸ºä¼—äººç©ç›®çš„ç„¦ç‚¹äººå£«å…¶ç‹¬ç‰¹çš„ä¸ªæ€§ä½¿å…¶æˆä¸ºäº†å½“ä»Šå¨±ä¹ç•Œå½“ä¹‹æ— æ„§çš„åäººå½¢è±¡æ›´æ˜¯å—åˆ°äº†äººä»¬çš„å…³æ³¨......å½“ç„¶ä½œä¸ºä¸€ä½æ°å‡ºçš„è‰ºæœ¯å®¶ä¹Ÿæ˜¯ä¼—äººçš„å¶åƒåŒæ—¶ä¹Ÿä»¥å…¶ä¸°å¯Œçš„æƒ³è±¡åŠ›å’Œè‰ºæœ¯æ‰åèµ¢å¾—äº†å¹¿å¤§è§‚ä¼—çš„ä¸€è‡´å¥½è¯„å¹¶ä¸”è¿˜è¢«èª‰ä¸ºéŸ³ä¹å¤©æ‰å‹è‰ºäººä»è€Œè·å¾—äº†æ— æ•°ä¸ªèµèª‰å¥–æ¯è£èª‰å¥–ç« å¥–é¡¹ç­‰è¯¸å¤šè£èª‰ç§°å·ä»¤ä¸–äººé—»é£çš†æƒŠç”šè‡³ç™»ä¸Šäº†å…¨çƒåª’ä½“å¤´æ¡ä»¥å¥¹çš„æ°å‡ºæˆå°±ä¸ä»…å¾—åˆ°äº†ä¸–ç•Œå„å›½æ”¿åºœæœ€é«˜é¢†å¯¼äººè‚¯å®šå¹¶è¢«æˆäºˆå¤šé¡¹æ®Šè£å‹‹ç« å¦‚è·å¾—è”åˆå›½æ•™ç§‘æ–‡ç»„ç»‡é¢å‘çš„å’Œå¹³æ•™è‚²å‘å±•ç‰¹æ®Šè´¡çŒ®å¥–é‡‘ç¾å›½å›½ä¼šé¢æˆå¥¹ç‰¹è®¸æ¼”å¥å‘˜ç§°å·å¹¶è·å¾—çš‡å®¶èŠ­è•¾èˆå›¢2008å¹´å¹´åº¦æ¼”å‡ºå¤§å¥–è¯ä¹¦éŸ©å›½æ€»ç»Ÿé¢å¸ƒçºªå¿µç« é‡‘ï¼æ©è¾¾é¢å‘å›½é™…åèª‰æ•™æˆèµ„æ ¼ç»ˆèº«æ•™èŒè¯é¦™æ¸¯ç‰¹åŒºç«‹æ³•ä¼šè®®é€šè¿‡ã€Šå® ç‰©ä¿æŠ¤æ³•ã€‹è·èµ å¥¥è¿é‡‘ç‰Œå¥–ç‰Œå…¨ä¸–ç•Œçš„çŸ¥åå½±æ˜Ÿå¯¼æ¼”ä½œå®¶åäººæ‚å¿—æŠ¥çº¸çº·çº·åˆŠç™»å¤§å¹…å¹¿å‘Šå®£ä¼ æŠ¥é“ç§°å¥¹æ˜¯ä¸–ç•Œä¸Šæœ€ä¼˜ç§€çš„æ¼”å‘˜æ­Œå”±å®¶å’Œæœ€å…·å½±å“åŠ›çš„æ­Œæ‰‹ä¸“å®¶çº§äººç‰©å› å¥¹åœ¨å½±è§†æ­Œå‰§é¢†åŸŸæ‰€åšå‡ºçš„çªå‡ºæˆç»©è€Œè¢«èª‰ä¸ºäºšæ´²æ¼”è‰ºå·¨æ˜Ÿä¸­å›½ç”µå½±åä¼šä¸»å¸­æå®‰å…ˆç”Ÿå°†ä¸ºå¥¹é¢å¥–è¡¨å½°ä¸ºä¸­å›½åšå‡ºå·¨å¤§å¥‰çŒ®çš„ä¸€ä»½ä»½è£è€€æ„Ÿå¬ç€å¥¹å§‹ç»ˆå¥‹æ–—å‰è¿›é“è·¯ä¸Šçš„è‰°è¾›é™©é˜»è€Œä¸æ€•å›°éš¾æŒ«æŠ˜æ¯«ä¸é€€ç¼©åšå¼ºä¸å±ˆå‹‡æ•¢æ— ç•ä¹è§‚å‘ä¸Šä¸æ–­è¿›å–æ°¸ä¸æœè¾“æ„å¿—åšå®šåšéŸ§ä¸æ‡ˆç§¯ææ¢ç´¢äººç”Ÿè¿½æ±‚æ¢¦æƒ³ç†æƒ³ä¿¡å¿µæ°¸è¿œå‘å‰çœ‹æ°¸ä¸è¨€å¼ƒä¸å¿˜åˆå¿ƒç‰¢è®°ä½¿å‘½ç»§ç»­å‰è¡Œå…±åŒåŠªåŠ›ç”¨æ±—æ°´æµ‡çŒå¸Œæœ›ç”¨å¿ƒè¡€æŸ“çº¢äº†ç”Ÿå‘½çƒ­æƒ…æ¾æ¹ƒæ²¸è…¾çš„çƒ­çˆ±æ¿€æƒ…ç‡ƒçƒ§çš„é«˜æ½®æœŸå……æ»¡åå·å´å²–é£é›¨é£˜æ‘‡è‰°éš¾å›°è‹¦æ—¶åˆ»è€ƒéªŒæˆ‘ä»¬è¦è¿æ¥æŒ‘æˆ˜å‹‡å¾€ç›´å‰æˆ˜èƒœä¸€ä¸ªä¸ªéš¾å…³åªè¦æœ‰äº†åšå®šçš„ç›®æ ‡ç›¸ä¿¡å°±ä¼šæˆåŠŸåªè¦æœ‰ä¿¡å¿ƒæ»¡æ€€è±ªæƒ…å®šç„¶æˆäº‹ç«‹ä¸šè§£ç­”é¢˜å¦‚ä¸‹:\n1-3åˆ†ç­”æ­£ç¡®2ï¼‰4ã€‘5ã€6ã€7ã€9****10`11``12**13/14^15#16$17%18Ã·19<20>21ï¼œ22ï¼23=24â†25Ã—26@27â–²28â†“29ã€ˆ30ã€‰31â–¼32â‰¤33â‰¥34&35ï½œ36ï½37âˆš38â†‘39â–¡40â—‹41â—42â”‚43\\*44ï¼45â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©ã€ï¼ˆã€–ï¼ƒä¸Šæ ‡ä¸‹åˆ’çº¿ä¹‹é—´å†…å®¹å¯ä»¥çœç•¥ä¸è®¡å†™æ•°å­—éƒ¨åˆ†ã€•ã€—ç¬¦å·å†…æ–‡å­—ç”±å­—æ¯æˆ–æ±‰å­—ç»„æˆæŒ‰é˜¿æ‹‰ä¼¯æ•°ä¹¦å†™æ’åˆ—é¡ºåºä¾æ¬¡è¿›è¡Œç¼–å·å³ç¬¬ä¸€è¡Œä¸ºç¬¬ï¼‘ä½æ¬¡åºï¼›ç¬¬äºŒè‡³ç¬¬å››äº”å…­ä¸ƒå…«ä¹ååä¸€åäºŒåä¸‰åå››åäº”åå…­åä¸ƒåå…«åä¹äºŒååäºŒåä¸€ç¬¬åäºŒåä¸ºç¬¬åä¸‰ä½ä»¥æ­¤ç±»æ¨[ç¼–è¾‘]ä»¥ä¸‹ä¸ºåŸç­”æ¡ˆè§£æ]\n0   A B C D E F G H I J K L M N O P Q R S T U V W X Y Z AA AB AC AD AE AF AG ABO APP BA BB BC BD BE BF BG BO BP BV BW BY WW XX YY CC CD DE EF GH AI IM JP QB QC RC RD RE RF RG RI RO SQ SR ST SU TV TW TX UV VW UX XV VI VII VIII IX XI Xi XL MM NN ON OP OM MR RM OR OS OT OC PO PR PS RT TS US AU AV AT AW AX AZ AM AN BN NH CN DN NC NT NO NP NW NX NY NZ NM NS OW PW RX RW SW SX SY TT TR TM TN TO TP PT PL PM MN PX PY PB PD PE PF PG PH PI PK LP KN KB KC KS LD LE LM ML MP MS MT MX MV MW MY MA NA ND NE NI IN IP IB IO IS IT IE IL SI SL SN SM SP SS SV SF SG SH II III IV VA VL VP VM WM WP CW CX DX EP EM EN EB EC ED EG ET ER ES EE EU EL UM ME MG MH MI MU MD MC MO FM MF NF FN FT GN GM GO GW GU GT GG GL TG GI GR GE GD GB GP GF FG FO FP FR FS GS JS JC JD GA GC DG DF DC DM DO DP DR DS DT DL DU DW DD DK DI DJ DV FW FF FY FL FA FB FC FE CF CM CP CE CK CL CT CU CV CR CA CB DA DB UC SC SD TD TH TI TF TB TC TL TK UL KM LS LT LV LR LI LG LO LL LC LA LB LF BL BM BS BT BU BR RS RR RL RP NR RB PC PV PP PQ QR RA RV VR WR WT WC WB WE WS WH WI WL LW BI IBM HP HL HF HM HD HB HE HT HH HV HC HI HK IH IC IG IK IA ID IF CI CJ CO CG CS CH HO CANONSONSIAUINOTHEMANYEARTHIANCOSPANICATURBISTARSHIAPETROGASTORIOUGEOSEACOMMONWEALTHOFTHEISLANDANDFUTUREPEOPLEFORREALITYWORLDWINNERSIGNIFACTIVELYCHIEFFINGTHEDIRICHARDPRICEWHILENOTEVERYONEABOUTMEBEFORENOWAYELONGTHERESTERPARTNERDOESNTREPEATMYVOCALYSOFTERMUSAGEITTOODONTUSEUPTHISTEXTSOLOVERALLYOUCANSEEADREAMSTRIKEWITHAMIDENYLIGHTDIGITALTELEVISIONPRESENTASLARGEPIANOFLUIDRADIOMASTERPHOTOGRAPHIESOUNDSYSTEMSMUSICCOMPUTERCLOCKWISEPOWERPOINTMANAGEROPTIONALESSAGESQUESPASSIONATEKISSABLEFOXPROFILESERIALNOISEEXPERIMENTAECONOMICDEMOGENERICSCOUNTRIPLETOWNINGSIDE OFHOSESANTHRILLFRUITJUNKPOCKETSFAZADEPARALLELOGRAMMERUNDERNEIGHBLENDERRAINMAPPAINTBRIDGEINSPECTORSOURIERULEMARKOVARIANSOLANGEVENESSIBLYTERNSUBJECTIVECREATORISHOSTAILSALESFEATURECOOKTOPAIRPLANECELLULARARENAUTOBAUDROMATICOPERAINDUSTRAILCHEETAHEADLIGHTSBICYLEDUCATIONCORPORAUTICSUMMARIZATIONSKIFFQUAKEMIXDRINKSLAWMOREGRACEFULLIZETHANKSGENERATEDMEMORYPACKATTACKSENSITIVEBACKENDISTRIBUTOREMAILSUBMITTINGREFERENCESBYCOLLABRATIONLINKSYSVENTURESAVAILABLEDOWNLOADURLhttp://www.tapetree.com/Solutions/TapeTree/Browse.aspx?id=\"01\"\nbcccaaaaabcddeeefghhhiiijjjkkllmmpprrssttuuvwxzzzwwyyzaaaabcdefghijklmnopqqrsuuuyyyyxyzdefgjklmnpqstvxyyzxabcdeffggjklmnoppqrstuvwxxzyy...............aabbcccddddd.........aaaaeeeeddadaahhhiiiiilnnoooottttsssuuxwxxxyyyyxxxxxxxxxxxxxcbbbbbbaadfgrklooiimmppttxzwxfoggiiklmlpqpseriyaouaxeeggbchihliirshuhaiivvywhomrsvtyawewcbsudswgfduysacagfhfgdiymphrethixcvfyzhaoiyauicgninnoovskidquayugoyeyamnsiwecpsiqsrgtspxoobflmdmsceebcttcibhsedgsytboftsgexerlebrblsnrtuppyylldossebtwdgtfoekozhttwczgykrksrqsmnmocrdlyfeclslktatottdglscsfkdtpckszemtkryteoxazofyxowndoravrlmt"
      },
      "finish_reason": "length"
    }
  ],
  "usage": {
    "prompt_tokens": 8,
    "total_tokens": 2048,
    "completion_tokens": 2040
  }
}
```

### gradio

> ä¸å…³é—­serverï¼Œç›´æ¥å¯åŠ¨gradioå‰ç«¯

```sh
lmdeploy serve gradio http://localhost:23333 \
    --server-name 0.0.0.0 \
    --server-port 6006
```

> æˆ–è€…ç›´æ¥å¯åŠ¨gradio

```sh
lmdeploy serve gradio \
    ./models/internlm2-chat-1_8b-4bit \
    --backend turbomind \
    --model-format awq \
    --tp 1 \
    --cache-max-entry-count 0.4 \
    --quant-policy 0 \
    --server-name 0.0.0.0 \
    --server-port 6006
```

> è¿œç¨‹è¿æ¥

```sh
ssh -CNg -L 6006:127.0.0.1:6006 root@ssh.intern-ai.org.cn -p 40165
```

> è®¿é—® `127.0.0.1:6006`

![image-20240409145723912](../attachment/InternLM2_homework5.assets/gradio.png)

## pythonä»£ç è¿è¡Œé‡åŒ–æ¨¡å‹

ä½¿ç”¨W4A16é‡åŒ–ï¼Œè°ƒæ•´KV Cacheçš„å ç”¨æ¯”ä¾‹ä¸º0.4ï¼Œä½¿ç”¨Pythonä»£ç é›†æˆçš„æ–¹å¼è¿è¡Œinternlm2-chat-1.8bæ¨¡å‹ã€‚

```python
from lmdeploy import pipeline, GenerationConfig, TurbomindEngineConfig, ChatTemplateConfig


if __name__ == '__main__':
    # å¯ä»¥ç›´æ¥ä½¿ç”¨transformersçš„æ¨¡å‹,ä¼šè‡ªåŠ¨è½¬æ¢æ ¼å¼
    # https://lmdeploy.readthedocs.io/zh-cn/latest/api/pipeline.html#turbomindengineconfig
    backend_config = TurbomindEngineConfig(
        model_name = 'internlm2',
        model_format = 'awq', # The format of input model. `hf` meaning `hf_llama`, `llama` meaning `meta_llama`, `awq` meaning the quantized model by awq. Default: None. Type: str
        tp = 1,
        session_len = 2048,
        max_batch_size = 128,
        cache_max_entry_count = 0.4, # è°ƒæ•´KV Cacheçš„å ç”¨æ¯”ä¾‹ä¸º0.4
        cache_block_seq_len = 64,
        quant_policy = 4, # é»˜è®¤ä¸º0, 4ä¸ºå¼€å¯kvcache int8 é‡åŒ–
        rope_scaling_factor = 0.0,
        use_logn_attn = False,
        download_dir = None,
        revision = None,
        max_prefill_token_num = 8192,
    )
    # https://lmdeploy.readthedocs.io/zh-cn/latest/_modules/lmdeploy/model.html#ChatTemplateConfig
    chat_template_config = ChatTemplateConfig(
        model_name = 'internlm2',
        system = None,
        meta_instruction = None,
    )
    # https://lmdeploy.readthedocs.io/zh-cn/latest/api/pipeline.html#generationconfig
    gen_config = GenerationConfig(
        n = 1,
        max_new_tokens = 1024,
        top_p = 0.8,
        top_k = 40,
        temperature = 0.8,
        repetition_penalty = 1.0,
        ignore_eos = False,
        random_seed = None,
        stop_words = None,
        bad_words = None,
        min_new_tokens = None,
        skip_special_tokens = True,
    )

    # https://lmdeploy.readthedocs.io/zh-cn/latest/api/pipeline.html
    # https://github.com/InternLM/lmdeploy/blob/main/lmdeploy/api.py
    pipe = pipeline(
        model_path = './models/internlm2-chat-1_8b-4bit', # W4A16
        model_name = 'internlm2_chat_1_8b',
        backend_config = backend_config,
        chat_template_config = chat_template_config,
    )

    #----------------------------------------------------------------------#
    # prompts (List[str] | str | List[Dict] | List[Dict]): a batch of
    #     prompts. It accepts: string prompt, a list of string prompts,
    #     a chat history in OpenAI format or a list of chat history.
    #----------------------------------------------------------------------#
    prompts = [[{
        'role': 'user',
        'content': 'Hi, pls intro yourself'
    }], [{
        'role': 'user',
        'content': 'Shanghai is'
    }]]

    # https://github.com/InternLM/lmdeploy/blob/main/lmdeploy/serve/async_engine.py#L274
    responses = pipe(prompts, gen_config=gen_config)
    for response in responses:
        print(response)
        print('text:', response.text)
        print('generate_token_len:', response.generate_token_len)
        print('input_token_len:', response.input_token_len)
        print('session_id:', response.session_id)
        print('finish_reason:', response.finish_reason)
        print()
```

> è¿è¡Œå‘½ä»¤è®°å½•

```sh
> python turbomind_pipeline.py
[WARNING] gemm_config.in is not found; using default GEMM algo
Response(text="Hello! My name is InternLM (ä¹¦ç”ŸÂ·æµ¦è¯­), and I am a language model designed to assist and provide information to users. I'm here to help you with any questions or tasks you may have. I'm here to provide honest and helpful responses, and I'm committed to ensuring that my responses are safe and harmless. Please feel free to ask me anything, and I'll do my best to assist you.", generate_token_len=87, input_token_len=108, session_id=0, finish_reason='stop')
text: Hello! My name is InternLM (ä¹¦ç”ŸÂ·æµ¦è¯­), and I am a language model designed to assist and provide information to users. I'm here to help you with any questions or tasks you may have. I'm here to provide honest and helpful responses, and I'm committed to ensuring that my responses are safe and harmless. Please feel free to ask me anything, and I'll do my best to assist you.
generate_token_len: 87
input_token_len: 108
session_id: 0
finish_reason: stop

Response(text='å¥½çš„ï¼Œæˆ‘å¯ä»¥å¸®ä½ äº†è§£ä¸Šæµ·ã€‚ä¸Šæµ·æ˜¯ä½äºä¸­å›½åä¸œåœ°åŒºçš„çœçº§åŸå¸‚ï¼Œæ˜¯é•¿æ±Ÿä¸‰è§’æ´²åŸå¸‚ç¾¤çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚ä¸Šæµ·æ˜¯ä¸­å›½çš„ç»æµä¸­å¿ƒã€ç§‘æŠ€åˆ›æ–°ä¸­å¿ƒå’Œå›½é™…å¤§éƒ½å¸‚ï¼Œä¹Ÿæ˜¯ä¸Šæµ·è‡ªç”±è´¸æ˜“åŒºçš„é‡è¦åŸºåœ°ã€‚ä¸Šæµ·æ‹¥æœ‰ä¸–ç•Œè‘—åçš„æ—…æ¸¸æ™¯ç‚¹å’Œå†å²æ–‡åŒ–é—äº§ï¼Œå¦‚å¤–æ»©ã€è±«å›­ã€ä¸Šæµ·åšç‰©é¦†ç­‰ã€‚ä¸Šæµ·ä¹Ÿæ˜¯ä¸­å›½çš„æ–‡åŒ–ä¸­å¿ƒï¼Œæ‹¥æœ‰ä¸°å¯Œçš„æ–‡åŒ–æ´»åŠ¨å’Œè‰ºæœ¯è¡¨æ¼”ã€‚ä¸Šæµ·è¿˜æ‹¥æœ‰å›½é™…ä¸€æµçš„æ•™è‚²æœºæ„å’Œç ”ç©¶æœºæ„ï¼Œå¦‚ä¸Šæµ·äº¤é€šå¤§å­¦ã€åä¸œå¸ˆèŒƒå¤§å­¦ç­‰ã€‚ä¸Šæµ·è¿˜æ‹¥æœ‰å‘è¾¾çš„äº¤é€šç½‘ç»œï¼ŒåŒ…æ‹¬åœ°é“ã€å…¬äº¤ã€è½»è½¨ç­‰ï¼Œæ–¹ä¾¿äººä»¬å‡ºè¡Œã€‚ä¸Šæµ·ä¹Ÿæ˜¯ä¸­å›½æœ€å›½é™…åŒ–çš„åŸå¸‚ä¹‹ä¸€ï¼Œæ‹¥æœ‰ä¸°å¯Œçš„æ—…æ¸¸ã€å•†ä¸šã€æ–‡åŒ–ã€ç§‘æŠ€ã€æ•™è‚²ç­‰æ–¹é¢çš„æœºä¼šå’Œèµ„æºã€‚', generate_token_len=144, input_token_len=105, session_id=1, finish_reason='stop')
text: å¥½çš„ï¼Œæˆ‘å¯ä»¥å¸®ä½ äº†è§£ä¸Šæµ·ã€‚ä¸Šæµ·æ˜¯ä½äºä¸­å›½åä¸œåœ°åŒºçš„çœçº§åŸå¸‚ï¼Œæ˜¯é•¿æ±Ÿä¸‰è§’æ´²åŸå¸‚ç¾¤çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚ä¸Šæµ·æ˜¯ä¸­å›½çš„ç»æµä¸­å¿ƒã€ç§‘æŠ€åˆ›æ–°ä¸­å¿ƒå’Œå›½é™…å¤§éƒ½å¸‚ï¼Œä¹Ÿæ˜¯ä¸Šæµ·è‡ªç”±è´¸æ˜“åŒºçš„é‡è¦åŸºåœ°ã€‚ä¸Šæµ·æ‹¥æœ‰ä¸–ç•Œè‘—åçš„æ—…æ¸¸æ™¯ç‚¹å’Œå†å²æ–‡åŒ–é—äº§ï¼Œå¦‚å¤–æ»©ã€è±«å›­ã€ä¸Šæµ·åšç‰©é¦†ç­‰ã€‚ä¸Šæµ·ä¹Ÿæ˜¯ä¸­å›½çš„æ–‡åŒ–ä¸­å¿ƒï¼Œæ‹¥æœ‰ä¸°å¯Œçš„æ–‡åŒ–æ´»åŠ¨å’Œè‰ºæœ¯è¡¨æ¼”ã€‚ä¸Šæµ·è¿˜æ‹¥æœ‰å›½é™…ä¸€æµçš„æ•™è‚²æœºæ„å’Œç ”ç©¶æœºæ„ï¼Œå¦‚ä¸Šæµ·äº¤é€šå¤§å­¦ã€åä¸œå¸ˆèŒƒå¤§å­¦ç­‰ã€‚ä¸Šæµ·è¿˜æ‹¥æœ‰å‘è¾¾çš„äº¤é€šç½‘ç»œï¼ŒåŒ…æ‹¬åœ°é“ã€å…¬äº¤ã€è½»è½¨ç­‰ï¼Œæ–¹ä¾¿äººä»¬å‡ºè¡Œã€‚ä¸Šæµ·ä¹Ÿæ˜¯ä¸­å›½æœ€å›½é™…åŒ–çš„åŸå¸‚ä¹‹ä¸€ï¼Œæ‹¥æœ‰ä¸°å¯Œçš„æ—…æ¸¸ã€å•†ä¸šã€æ–‡åŒ–ã€ç§‘æŠ€ã€æ•™è‚²ç­‰æ–¹é¢çš„æœºä¼šå’Œèµ„æºã€‚
generate_token_len: 144
input_token_len: 105
session_id: 1
finish_reason: stop
```

![](../attachment/InternLM2_homework5.assets/chat3.png)

## LMDeploy è¿è¡Œ llava

ä½¿ç”¨ LMDeploy è¿è¡Œè§†è§‰å¤šæ¨¡æ€å¤§æ¨¡å‹ llava gradio demo

### å‘½ä»¤è¿è¡Œ

```python
from lmdeploy import pipeline, GenerationConfig, TurbomindEngineConfig, ChatTemplateConfig
from lmdeploy.vl import load_image

backend_config = TurbomindEngineConfig(
    cache_max_entry_count = 0.4, # è°ƒæ•´KV Cacheçš„å ç”¨æ¯”ä¾‹ä¸º0.4
)

# pipe = pipeline('liuhaotian/llava-v1.6-vicuna-7b') éå¼€å‘æœºè¿è¡Œæ­¤å‘½ä»¤
pipe = pipeline(
    '/share/new_models/liuhaotian/llava-v1.6-vicuna-7b',
    backend_config = backend_config,
)

image = load_image('https://raw.githubusercontent.com/open-mmlab/mmdeploy/main/tests/data/tiger.jpeg')
response = pipe(('describe this image', image))
print(response)
```

> è¿è¡Œå‘½ä»¤è®°å½•

```sh
> python pipeline_llava.py
[WARNING] gemm_config.in is not found; using default GEMM algo
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  4.24it/s]
Response(text="\n\nThis image features a tiger lying down on what appears to be a grassy area. The tiger is facing the camera with its head slightly tilted to the side, displaying its distinctive orange and black stripes. Its eyes are open and it seems to be looking directly at the viewer, giving a sense of engagement. The tiger's fur is in focus, with the stripes clearly visible, while the background is slightly blurred, which puts the emphasis on the tiger. The lighting suggests it might be a sunny day, as the tiger's fur is highlighted in areas that are not in direct sunlight. There are no texts or other objects in the image. The style of the photograph is a naturalistic wildlife shot, capturing the tiger in its environment.", generate_token_len=172, input_token_len=1023, session_id=0, finish_reason='stop')
```

![](../attachment/InternLM2_homework5.assets/llava1.png)



### gradio

```python
import gradio as gr
from lmdeploy import pipeline, GenerationConfig, TurbomindEngineConfig, ChatTemplateConfig

backend_config = TurbomindEngineConfig(
    cache_max_entry_count = 0.4, # è°ƒæ•´KV Cacheçš„å ç”¨æ¯”ä¾‹ä¸º0.4
)

# pipe = pipeline('liuhaotian/llava-v1.6-vicuna-7b') éå¼€å‘æœºè¿è¡Œæ­¤å‘½ä»¤
pipe = pipeline(
    '/share/new_models/liuhaotian/llava-v1.6-vicuna-7b',
    backend_config = backend_config,
)

def model(image, text):
    if image is None:
        return [(text, "è¯·ä¸Šä¼ ä¸€å¼ å›¾ç‰‡ã€‚")]
    else:
        response = pipe((text, image)).text
        return [(text, response)]

demo = gr.Interface(fn=model, inputs=[gr.Image(type="pil"), gr.Textbox()], outputs=gr.Chatbot())
demo.launch()
```

> è¿œç¨‹è¿æ¥

```sh
ssh -CNg -L 7860:127.0.0.1:7860 root@ssh.intern-ai.org.cn -p 40165
```

> è®¿é—® `127.0.0.1:7860`
>
> æé—®æ˜¯ `introduce this image`, ç»“æœè¿”å›äº†æ—¥è¯­

![image-20240409172940015](../attachment/InternLM2_homework5.assets/llava2.png)

> æé—®æ”¹ä¸º `introduce this image in english`, è¿”å›è‹±è¯­

![](../attachment/InternLM2_homework5.assets/llava3.png)

### é«˜åˆ†è¾¨ç‡å›¾ç‰‡é—®é¢˜

> å·²æäº¤bugå¹¶è¢«å®˜æ–¹è§£å†³
>
> [camp2 lmdeploy llavaè¿è¡Œæ—¶è¾“å…¥é«˜åˆ†è¾¨ç‡å›¾ç‰‡ä¼šè¿”å›ç©ºå­—ç¬¦ä¸²](https://github.com/InternLM/Tutorial/issues/620)
>
> [ä¿®å¤äº†é«˜åˆ†è¾¨ç‡å›¾åƒllavaè¾“å‡ºä¸ºç©ºçš„bug #620 (#623)](https://github.com/InternLM/Tutorial/commit/3b54212219569b09a7c8a7955cb413f5dd08ad6e)
>
> è§£å†³æ–¹æ³•å¦‚ä¸‹ï¼Œè§£å†³æ–¹æ³•æ˜¯è°ƒé«˜session_len

```python
from lmdeploy import pipeline, TurbomindEngineConfig


backend_config = TurbomindEngineConfig(session_len=8192) # å›¾ç‰‡åˆ†è¾¨ç‡è¾ƒé«˜æ—¶è¯·è°ƒé«˜session_len
# pipe = pipeline('liuhaotian/llava-v1.6-vicuna-7b', backend_config=backend_config) éå¼€å‘æœºè¿è¡Œæ­¤å‘½ä»¤
pipe = pipeline('/share/new_models/liuhaotian/llava-v1.6-vicuna-7b', backend_config=backend_config)
```

> å½“å›¾ç‰‡åˆ†è¾¨ç‡è¾ƒé«˜æ—¶ï¼Œè¾“å‡ºtextä¸ºç©º
>
> å³ä¾§è¾“å‡ºä¸ºç©º

![image-20240409173516321](../attachment/InternLM2_homework5.assets/llava4.png)

> æ‰“å° `response.text` ä¸ºç©º



![llava5](../attachment/InternLM2_homework5.assets/llava5.png)

> è§£å†³åŠæ³•ä¸ºé™ä½åˆ†è¾¨ç‡

```python
import gradio as gr
from lmdeploy import pipeline


# pipe = pipeline('liuhaotian/llava-v1.6-vicuna-7b') éå¼€å‘æœºè¿è¡Œæ­¤å‘½ä»¤
pipe = pipeline('/share/new_models/liuhaotian/llava-v1.6-vicuna-7b')

def model(image, text):
    if image is None:
        return [(text, "è¯·ä¸Šä¼ ä¸€å¼ å›¾ç‰‡ã€‚")]
    else:
        width, height = image.size
        print(f"width = {width}, height = {height}")

        # è°ƒæ•´å›¾ç‰‡æœ€é•¿å®½/é«˜ä¸º256
        if max(width, height) > 256:
            ratio = max(width, height) / 256
            n_width = int(width / ratio)
            n_height = int(height / ratio)
            print(f"new width = {n_width}, new height = {n_height}")
            image = image.resize((n_width, n_height))

        response = pipe((text, image)).text
        print(f"response: {response}")
        return [(text, response)]

demo = gr.Interface(fn=model, inputs=[gr.Image(type="pil"), gr.Textbox()], outputs=gr.Chatbot())
demo.launch()
```

> è°ƒæ•´åˆ†è¾¨ç‡åå¯ä»¥æ­£å¸¸è¿è¡Œ

![](../attachment/InternLM2_homework5.assets/llava6.png)

> è°ƒæ•´åˆ†è¾¨ç‡åå¯ä»¥æ­£å¸¸è¿è¡Œ

![](../attachment/InternLM2_homework5.assets/llava7.png)

## å°† LMDeploy Web Demo éƒ¨ç½²åˆ° [OpenXLab](https://github.com/InternLM/Tutorial/blob/camp2/tools/openxlab-deploy)

é¡¹ç›®åœ°å€ https://openxlab.org.cn/apps/detail/NagatoYuki0943/LMDeployWebDemobyNagatoYuki0943

ä»“åº“åœ°å€ https://github.com/NagatoYuki0943/LMDeploy-Web-Demo

æ ¹æ®è¦æ±‚åˆ›å»ºä»“åº“å’Œå¯¹åº”æ–‡ä»¶

```sh
â”œâ”€GitHub_Repo_Name
â”‚  â”œâ”€app.py                 # Gradio åº”ç”¨é»˜è®¤å¯åŠ¨æ–‡ä»¶ä¸ºapp.pyï¼Œåº”ç”¨ä»£ç ç›¸å…³çš„æ–‡ä»¶åŒ…å«æ¨¡å‹æ¨ç†ï¼Œåº”ç”¨çš„å‰ç«¯é…ç½®ä»£ç 
â”‚  â”œâ”€requirements.txt       # å®‰è£…è¿è¡Œæ‰€éœ€è¦çš„ Python åº“ä¾èµ–ï¼ˆpip å®‰è£…ï¼‰
â”‚  â”œâ”€packages.txt           # å®‰è£…è¿è¡Œæ‰€éœ€è¦çš„ Debian ä¾èµ–é¡¹ï¼ˆ apt-get å®‰è£…ï¼‰
|  â”œâ”€README.md              # ç¼–å†™åº”ç”¨ç›¸å…³çš„ä»‹ç»æ€§çš„æ–‡æ¡£
â”‚  â””â”€...
```

`packages.txt` æ·»åŠ éœ€è¦çš„dibianä¾èµ–

```sh
git
git-lfs
```

`requirements.txt` ä¸­æ·»åŠ éœ€è¦çš„pythonä¾èµ–

```txt
gradio>4
transformers
sentencepiece
einops
accelerate
tiktoken
lmdeploy==0.3.0
```

`app.py` ä¸­ç¼–å†™ä»£ç 

ä¸»è¦å†…å®¹æœ‰ä¸‹è½½æ¨¡å‹ï¼Œè½½å…¥æ¨¡å‹ï¼Œå¯åŠ¨gradio

```python
import os
import gradio as gr
import lmdeploy
from lmdeploy import pipeline, GenerationConfig, TurbomindEngineConfig, ChatTemplateConfig
from typing import Generator, Any


print("lmdeploy version: ", lmdeploy.__version__)
print("gradio version: ", gr.__version__)


# clone æ¨¡å‹
model_path = './models/internlm2-chat-1_8b'
os.system(f'git clone https://code.openxlab.org.cn/OpenLMLab/internlm2-chat-1.8b {model_path}')
os.system(f'cd {model_path} && git lfs pull')

# å¯ä»¥ç›´æ¥ä½¿ç”¨transformersçš„æ¨¡å‹,ä¼šè‡ªåŠ¨è½¬æ¢æ ¼å¼
# https://lmdeploy.readthedocs.io/zh-cn/latest/api/pipeline.html#turbomindengineconfig
backend_config = TurbomindEngineConfig(
    model_name = 'internlm2',
    model_format = 'hf', # The format of input model. `hf` meaning `hf_llama`, `llama` meaning `meta_llama`, `awq` meaning the quantized model by awq. Default: None. Type: str
    tp = 1,
    session_len = 2048,
    max_batch_size = 128,
    cache_max_entry_count = 0.8, # è°ƒæ•´KV Cacheçš„å ç”¨æ¯”ä¾‹ä¸º0.8
    cache_block_seq_len = 64,
    quant_policy = 0, # é»˜è®¤ä¸º0, 4ä¸ºå¼€å¯kvcache int8 é‡åŒ–
    rope_scaling_factor = 0.0,
    use_logn_attn = False,
    download_dir = None,
    revision = None,
    max_prefill_token_num = 8192,
)

system_prompt = """You are an AI assistant whose name is InternLM (ä¹¦ç”ŸÂ·æµ¦è¯­).
- InternLM (ä¹¦ç”ŸÂ·æµ¦è¯­) is a conversational language model that is developed by Shanghai AI Laboratory (ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤). It is designed to be helpful, honest, and harmless.
- InternLM (ä¹¦ç”ŸÂ·æµ¦è¯­) can understand and communicate fluently in the language chosen by the user such as English and ä¸­æ–‡.
"""

# https://lmdeploy.readthedocs.io/zh-cn/latest/_modules/lmdeploy/model.html#ChatTemplateConfig
chat_template_config = ChatTemplateConfig(
    model_name = 'internlm2',
    system = None,
    meta_instruction = system_prompt,
)

# https://lmdeploy.readthedocs.io/zh-cn/latest/api/pipeline.html#generationconfig
gen_config = GenerationConfig(
    n = 1,
    max_new_tokens = 1024,
    top_p = 0.8,
    top_k = 40,
    temperature = 0.8,
    repetition_penalty = 1.0,
    ignore_eos = False,
    random_seed = None,
    stop_words = None,
    bad_words = None,
    min_new_tokens = None,
    skip_special_tokens = True,
)

# https://lmdeploy.readthedocs.io/zh-cn/latest/api/pipeline.html
# https://github.com/InternLM/lmdeploy/blob/main/lmdeploy/api.py
# https://github.com/InternLM/lmdeploy/blob/main/lmdeploy/serve/async_engine.py
pipe = pipeline(
    model_path = model_path,
    model_name = 'internlm2_chat_1_8b',
    backend_config = backend_config,
    chat_template_config = chat_template_config,
)

#----------------------------------------------------------------------#
# prompts (List[str] | str | List[Dict] | List[Dict]): a batch of
#     prompts. It accepts: string prompt, a list of string prompts,
#     a chat history in OpenAI format or a list of chat history.
# [
#     {
#         "role": "system",
#         "content": "You are a helpful assistant."
#     },
#     {
#         "role": "user",
#         "content": "What is the capital of France?"
#     },
#     {
#         "role": "assistant",
#         "content": "The capital of France is Paris."
#     },
#     {
#         "role": "user",
#         "content": "Thanks!"
#     },
#     {
#         "role": "assistant",
#         "content": "You are welcome."
#     }
# ]
#----------------------------------------------------------------------#


def chat(
    query: str,
    history: list = [],  # [['What is the capital of France?', 'The capital of France is Paris.'], ['Thanks', 'You are Welcome']]
    max_new_tokens: int = 1024,
    top_p: float = 0.8,
    top_k: int = 40,
    temperature: float = 0.8,
    regenerate: str = "" # æ˜¯regenæŒ‰é’®çš„value,å­—ç¬¦ä¸²,ç‚¹å‡»å°±ä¼ é€,å¦åˆ™ä¸ºç©ºå­—ç¬¦ä¸²
) -> Generator[Any, Any, Any]:
    """èŠå¤©"""
    global gen_config

    # é‡æ–°ç”Ÿæˆæ—¶è¦æŠŠæœ€åçš„queryå’Œresponseå¼¹å‡º,é‡ç”¨query
    if regenerate:
        # æœ‰å†å²å°±é‡æ–°ç”Ÿæˆ,æ²¡æœ‰å†å²å°±è¿”å›ç©º
        if len(history) > 0:
            query, _ = history.pop(-1)
        else:
            yield history
            return # è¿™æ ·å†™ç®¡ç”¨,ä½†ä¸ç†è§£
    else:
        query = query.replace(' ', '')
        if query == None or len(query) < 1:
            yield history
            return

    # å°†å†å²è®°å½•è½¬æ¢ä¸ºopenaiæ ¼å¼
    prompts = []
    for user, assistant in history:
        prompts.append(
            {
                "role": "user",
                "content": user
            }
        )
        prompts.append(
            {
                "role": "assistant",
                "content": assistant
            })
    # éœ€è¦æ·»åŠ å½“å‰çš„query
    prompts.append(
        {
            "role": "user",
            "content": query
        }
    )

    # ä¿®æ”¹ç”Ÿæˆå‚æ•°
    gen_config.max_new_tokens = max_new_tokens
    gen_config.top_p = top_p
    gen_config.top_k = top_k
    gen_config.temperature = temperature
    print("gen_config: ", gen_config)

    # æ”¾å…¥ [{},{}] æ ¼å¼è¿”å›ä¸€ä¸ªresponse
    # æ”¾å…¥ [] æˆ–è€… [[{},{}]] æ ¼å¼è¿”å›ä¸€ä¸ªresponseåˆ—è¡¨
    print(f"query: {query}; response: ", end="", flush=True)
    response = ""
    for _response in pipe.stream_infer(
        prompts = prompts,
        gen_config = gen_config,
        do_preprocess = True,
        adapter_name = None
    ):
        # print(_response)
        # Response(text='å¾ˆé«˜å…´', generate_token_len=10, input_token_len=111, session_id=0, finish_reason=None)
        # Response(text='è®¤è¯†', generate_token_len=11, input_token_len=111, session_id=0, finish_reason=None)
        # Response(text='ä½ ', generate_token_len=12, input_token_len=111, session_id=0, finish_reason=None)
        print(_response.text, flush=True, end="")
        response += _response.text
        yield history + [[query, response]]
    print("\n")


def revocery(history: list = []) -> list:
    """æ¢å¤åˆ°ä¸Šä¸€è½®å¯¹è¯"""
    if len(history) > 0:
        history.pop(-1)
    return history


block = gr.Blocks()
with block as demo:
    with gr.Row(equal_height=True):
        with gr.Column(scale=15):
            gr.Markdown("""<h1><center>InternLM</center></h1>
                <center>InternLM2</center>
                """)
        # gr.Image(value=LOGO_PATH, scale=1, min_width=10,show_label=False, show_download_button=False)

    with gr.Row():
        with gr.Column(scale=4):
            # åˆ›å»ºèŠå¤©æ¡†
            chatbot = gr.Chatbot(height=500, show_copy_button=True)

            with gr.Row():
                max_new_tokens = gr.Slider(
                    minimum=1,
                    maximum=2048,
                    value=1024,
                    step=1,
                    label='Maximum new tokens'
                )
                top_p = gr.Slider(
                    minimum=0.01,
                    maximum=1,
                    value=0.8,
                    step=0.01,
                    label='Top_p'
                )
                top_k = gr.Slider(
                    minimum=1,
                    maximum=100,
                    value=40,
                    step=1,
                    label='Top_k'
                )
                temperature = gr.Slider(
                    minimum=0.01,
                    maximum=1.5,
                    value=0.8,
                    step=0.01,
                    label='Temperature'
                )

            with gr.Row():
                # åˆ›å»ºä¸€ä¸ªæ–‡æœ¬æ¡†ç»„ä»¶ï¼Œç”¨äºè¾“å…¥ promptã€‚
                query = gr.Textbox(label="Prompt/é—®é¢˜")
                # åˆ›å»ºæäº¤æŒ‰é’®ã€‚
                # variant https://www.gradio.app/docs/button
                # scale https://www.gradio.app/guides/controlling-layout
                submit = gr.Button("ğŸ’¬ Chat", variant="primary", scale=0)

            with gr.Row():
                # åˆ›å»ºä¸€ä¸ªé‡æ–°ç”ŸæˆæŒ‰é’®ï¼Œç”¨äºé‡æ–°ç”Ÿæˆå½“å‰å¯¹è¯å†…å®¹ã€‚
                regen = gr.Button("ğŸ”„ Retry", variant="secondary")
                undo = gr.Button("â†©ï¸ Undo", variant="secondary")
                # åˆ›å»ºä¸€ä¸ªæ¸…é™¤æŒ‰é’®ï¼Œç”¨äºæ¸…é™¤èŠå¤©æœºå™¨äººç»„ä»¶çš„å†…å®¹ã€‚
                clear = gr.ClearButton(components=[chatbot], value="ğŸ—‘ï¸ Clear", variant="stop")

        # å›è½¦æäº¤
        query.submit(
            chat,
            inputs=[query, chatbot, max_new_tokens, top_p, top_k, temperature],
            outputs=[chatbot]
        )

        # æ¸…ç©ºquery
        query.submit(
            lambda: gr.Textbox(value=""),
            [],
            [query],
        )

        # æŒ‰é’®æäº¤
        submit.click(
            chat,
            inputs=[query, chatbot, max_new_tokens, top_p, top_k, temperature],
            outputs=[chatbot]
        )

        # æ¸…ç©ºquery
        submit.click(
            lambda: gr.Textbox(value=""),
            [],
            [query],
        )

        # é‡æ–°ç”Ÿæˆ
        regen.click(
            chat,
            inputs=[query, chatbot, max_new_tokens, top_p, top_k, temperature, regen],
            outputs=[chatbot]
        )

        # æ’¤é”€
        undo.click(
            revocery,
            inputs=[chatbot],
            outputs=[chatbot]
        )

    gr.Markdown("""æé†’ï¼š<br>
    1. ä½¿ç”¨ä¸­å¦‚æœå‡ºç°å¼‚å¸¸ï¼Œå°†ä¼šåœ¨æ–‡æœ¬è¾“å…¥æ¡†è¿›è¡Œå±•ç¤ºï¼Œè¯·ä¸è¦æƒŠæ…Œã€‚<br>
    2. é¡¹ç›®åœ°å€ï¼šhttps://github.com/NagatoYuki0943/LMDeploy-Web-Demo
    """)

# threads to consume the request
gr.close_all()

# è®¾ç½®é˜Ÿåˆ—å¯åŠ¨ï¼Œé˜Ÿåˆ—æœ€å¤§é•¿åº¦ä¸º 100
demo.queue(max_size=100)


if __name__ == "__main__":
    # å¯åŠ¨æ–°çš„ Gradio åº”ç”¨ï¼Œè®¾ç½®åˆ†äº«åŠŸèƒ½ä¸º Trueï¼Œå¹¶ä½¿ç”¨ç¯å¢ƒå˜é‡ PORT1 æŒ‡å®šæœåŠ¡å™¨ç«¯å£ã€‚
    # demo.launch(share=True, server_port=int(os.environ['PORT1']))
    # ç›´æ¥å¯åŠ¨
    # demo.launch(server_name="127.0.0.1", server_port=7860)
    demo.launch()
```

åœ¨ openxlab https://openxlab.org.cn/home å®˜ç½‘ï¼Œç‚¹å‡»å³ä¸Šè§’çš„åˆ›å»ºæŒ‰é’®ï¼Œç‚¹å‡»åˆ›å»ºåº”ç”¨ï¼Œé€‰æ‹©gradioã€‚

![LMDeployWebDemo1](../attachment/InternLM2_homework5.assets/LMDeployWebDemo1.png)

å¡«å†™åº”ç”¨åç§°å’Œgithubåœ°å€ï¼Œé€‰æ‹©ç¡¬ä»¶èµ„æºå’Œé•œåƒã€‚

![LMDeployWebDemo2](../attachment/InternLM2_homework5.assets/LMDeployWebDemo2.png)

ç‚¹å‡»ç«‹å³åˆ›å»ºï¼Œå³å¯åˆ›å»ºåº”ç”¨ã€‚

![LMDeployWebDemo3](../attachment/InternLM2_homework5.assets/LMDeployWebDemo3.png)

ç»è¿‡é•¿æ—¶é—´ç­‰å¾…ï¼Œæ„å»ºæˆåŠŸï¼Œç­‰å¾…å¯åŠ¨ã€‚

![LMDeployWebDemo4](../attachment/InternLM2_homework5.assets/LMDeployWebDemo4.png)

å¯åŠ¨æˆåŠŸï¼Œå¯ä»¥å¯¹è¯ã€‚

![LMDeployWebDemo5](../attachment/InternLM2_homework5.assets/LMDeployWebDemo5.jpeg)